



# Explanations of the First 26 Functions in `run.py`

This document provides an in-depth breakdown of the first twenty-six functions in the `detect-gpt/run.py` file. Each function is explained in terms of its purpose, how it works, and its role in the overall pipeline.

---

## 1. `load_base_model()`
**Purpose:**  
Moves the base language model (e.g., GPT-2) to the GPU for faster computation, and moves the mask model (if it exists) back to CPU to free up GPU memory.

**How it works:**
- Prints a message indicating the base model is being moved to GPU.
- Records the start time for timing the operation.
- Tries to move the `mask_model` to CPU (if it exists) to free GPU memory. If `mask_model` is not defined, it ignores the error.
- If not using an OpenAI model (i.e., using a local model), moves `base_model` to the GPU device specified by `DEVICE`.
- Prints how long the operation took.

**Why:**  
Switching models between CPU and GPU helps manage limited GPU memory, especially when working with large models.

---

## 2. `load_mask_model()`
**Purpose:**  
Moves the mask-filling model (e.g., T5) to the GPU, and moves the base model back to CPU to save GPU memory.

**How it works:**
- Prints a message indicating the mask model is being moved to GPU.
- Records the start time.
- If not using an OpenAI model, moves the `base_model` to CPU to free up GPU memory.
- If not using random fills (i.e., actually using the mask model), moves `mask_model` to the GPU.
- Prints how long the operation took.

**Why:**  
This function is the counterpart to `load_base_model()`, ensuring only one large model is on the GPU at a time to avoid memory issues.

---

## 3. `tokenize_and_mask(text, span_length, pct, ceil_pct=False)`
**Purpose:**  
Randomly masks out spans of words in the input text, replacing them with special tokens (`<extra_id_0>`, `<extra_id_1>`, etc.) for use in span-masked language modeling (e.g., T5).

**How it works:**
1. **Tokenization:** Splits the input text into tokens (by spaces).
2. **Calculate Number of Spans:** Computes how many spans to mask based on the percentage (`pct`), the length of each span (`span_length`), and a buffer size. Optionally rounds up the number of spans if `ceil_pct` is `True`. Converts the number of spans to an integer.
3. **Masking Loop:** While the number of masked spans is less than the target, randomly selects a start index for a span, checks a buffer region around the span to ensure it doesn’t overlap with an existing mask, and if the region is clear, replaces the span with a placeholder (`<<<mask>>>`).
4. **Replace Placeholders with Special Tokens:** Iterates through the tokens, replacing each `<<<mask>>>` with a unique `<extra_id_N>` token, incrementing `N` for each mask. Asserts that the number of filled masks matches the intended number.
5. **Return:** Joins the tokens back into a string and returns the masked text.

**Why:**  
This prepares text for models like T5, which are trained to fill in masked spans denoted by `<extra_id_N>` tokens. The masking is random and non-overlapping, simulating the span corruption used in T5-style pretraining.

---

## 4. `count_masks(texts)`
**Purpose:**  
Counts the number of mask tokens (e.g., `<extra_id_0>`, `<extra_id_1>`, etc.) in each text in a list.

**How it works:**
- For each text in the input list `texts`, it splits the text into tokens (by spaces).
- It counts how many tokens start with `<extra_id_`.
- Returns a list of counts, one for each input text.

**Why:**  
This is used to determine how many masked spans are present in each text, which is important for later filling or processing steps.

---

## 5. `replace_masks(texts)`
**Purpose:**  
Fills in the masked spans in each text using the mask model (e.g., T5), generating new text with the masks replaced by model-generated content.

**How it works:**
- Uses `count_masks` to determine how many masks are in each text.
- Computes a `stop_id` so the model knows when to stop generating (based on the highest mask index).
- Tokenizes the input texts for the mask model.
- Calls the mask model’s `generate` method to fill in the masks, with sampling and stopping parameters.
- Decodes the generated outputs back into text, keeping special tokens.

**Why:**  
This function automates the process of filling in masked spans with plausible content, as part of the perturbation pipeline.

---

## 6. `extract_fills(texts)`
**Purpose:**  
Extracts the content generated by the mask model that fills in each masked span.

**How it works:**
- Cleans up each text by removing `<pad>` and `</s>` tokens and stripping whitespace.
- Uses a regex pattern to split the text at each `<extra_id_N>` token, extracting the segments between them (which are the fills).
- Removes leading/trailing whitespace from each fill.
- Returns a list of lists: each sublist contains the fills for one text.

**Why:**  
After the mask model generates text, this function isolates the actual content that was generated to fill each mask, so it can be inserted back into the original text.

---

## 7. `apply_extracted_fills(masked_texts, extracted_fills)`
**Purpose:**  
Replaces each `<extra_id_N>` mask token in the masked texts with the corresponding generated fill, reconstructing the perturbed text.

**How it works:**
- Splits each masked text into tokens.
- For each text, checks if the number of fills matches the number of masks.
- If so, replaces each mask token with its fill.
- Joins tokens back into a string and returns the list of perturbed texts.

**Why:**  
This function reconstructs the final perturbed text by inserting the generated content into the masked positions.

---

## 8. `perturb_texts_(texts, span_length, pct, ceil_pct=False)`
**Purpose:**  
Perturbs a batch of texts by masking and filling spans, either using a model or random fills.

**How it works:**
- If not using random fills:
	- Masks spans, generates fills, extracts them, and applies them.
	- Retries if the model fails to generate the correct number of fills.
- If using random fills:
	- Either replaces tokens with random tokens or fills masks with random words from a dictionary.

**Why:**  
This is the core function for generating perturbed versions of input texts for analysis.

---

## 9. `perturb_texts(texts, span_length, pct, ceil_pct=False)`
**Purpose:**  
Handles batching for perturbing large numbers of texts efficiently.

**How it works:**
- Splits the input texts into chunks (to avoid memory issues).
- Calls `perturb_texts_` on each chunk and aggregates the results.

**Why:**  
Allows the perturbation process to scale to large datasets by processing in manageable batches.

---

## 10. `drop_last_word(text)`
**Purpose:**  
Removes the last word from a given text string.

**How it works:**
- Splits the input text into words (by spaces).
- Removes the last word.
- Joins the remaining words back into a string.

**Why:**  
Used to trim prompts, especially before sending them to a language model for continuation.

---


## 12. `sample_from_model(texts, min_words=55, prompt_tokens=30)`
**Purpose:**  
Generates text samples from the base model (or OpenAI API) using the first `prompt_tokens` tokens of each input as context, ensuring each sample has at least `min_words` words.

**How it works:**
- For 'pubmed', trims each text at a separator and encodes for the model.
- For other datasets, encodes the first `prompt_tokens` tokens of each text.
- If using OpenAI, decodes prefixes and uses a thread pool to sample completions via `_openai_sample`.
- If using a local model, repeatedly samples until all outputs have at least `min_words` words, using top-p or top-k sampling if specified.
- Tracks API token usage if using OpenAI.
- Returns the generated samples.

**Why:**  
Ensures generated samples are long enough and uses only the beginning of each text as context, which is important for fair evaluation and for controlling the prompt length.

---

## 13. `get_likelihood(logits, labels)`
**Purpose:**  
Calculates the average log-likelihood of a sequence of labels (tokens) given the model’s output logits.

**How it works:**
- Checks that the batch size is 1.
- Reshapes logits and labels to align them for next-token prediction.
- Computes log-probabilities using softmax.
- Gathers the log-probabilities for the true labels.
- Returns the mean log-likelihood across the sequence.

**Why:**  
This is a standard metric for evaluating how well a model predicts a sequence, used for scoring text likelihoods in the pipeline.

---

## 14. `get_ll(text)`
**Purpose:**  
Computes the log-likelihood of a single text under the base model or OpenAI API.

**How it works:**
- If using OpenAI, sends the text as a prompt and retrieves log-probabilities for each token, then averages them.
- If using a local model, tokenizes the text and computes the negative loss (which is the log-likelihood) using the model in evaluation mode.
- Returns the mean log-likelihood.

**Why:**  
Provides a way to score texts for their likelihood under the model, which is central to DetectGPT’s analysis and discrimination between real and generated texts.

---

## 15. `get_lls(texts)`
**Purpose:**  
Computes the log-likelihoods for a list of texts under the base model or OpenAI API.

**How it works:**
- If not using OpenAI, calls `get_ll` for each text and returns the list of results.
- If using OpenAI, tracks API token usage, then uses a thread pool to compute log-likelihoods in parallel for all texts.
- Returns a list of log-likelihoods.

**Why:**  
Efficiently scores multiple texts, which is necessary for batch evaluation and analysis in experiments.

---

## 16. `get_rank(text, log=False)`
**Purpose:**  
Calculates the average rank of each observed token in the text, sorted by model likelihood.

**How it works:**
- Only works for local models (not OpenAI).
- Tokenizes the text and gets model logits.
- For each token, finds its rank in the model’s predicted likelihood ordering.
- Optionally applies a logarithm to the ranks.
- Returns the mean rank (or log-rank) across the sequence.

**Why:**  
Rank-based metrics help assess how well the model predicts the actual tokens, providing a different perspective than log-likelihood.

---

## 17. `get_entropy(text)`
**Purpose:**  
Computes the average entropy of the model’s token predictions for a given text.

**How it works:**
- Only works for local models.
- Tokenizes the text and gets model logits.
- Calculates the entropy for each token’s prediction (using softmax and log-softmax).
- Returns the mean entropy across the sequence.

**Why:**  
Entropy measures the uncertainty of the model’s predictions, which can be used to distinguish between real and generated texts.

---


## 19. `get_precision_recall_metrics(real_preds, sample_preds)`
**Purpose:**  
Calculates precision-recall curve metrics (precision, recall, and AUC) for distinguishing real vs. generated samples.

**How it works:**
- Combines real and sample predictions, labels them, and computes the precision-recall curve using scikit-learn.
- Calculates the area under the curve (AUC).
- Returns precision, recall, and AUC as lists and a float.

**Why:**  
Precision-recall metrics are useful for evaluating binary classifiers, especially when dealing with imbalanced datasets.

---

## 20. `save_roc_curves(experiments)`
**Purpose:**  
Saves ROC curve plots for each experiment, using colorblind-friendly colors.

**How it works:**
- Clears the current matplotlib plot.
- Iterates over experiments, plotting each ROC curve with a unique color and label.
- Adds a diagonal reference line, sets axis limits and labels, and saves the plot to a PNG file.

**Why:**  
Visualizes the performance of different experiments, making it easy to compare ROC curves and AUCs.

---

## 21. `save_ll_histograms(experiments)`
**Purpose:**  
Saves histograms of log likelihoods for real and perturbed texts, and for sampled and perturbed sampled texts.

**How it works:**
- Clears the current matplotlib plot.
- For each experiment, plots two side-by-side histograms: one for sampled/perturbed sampled, one for original/perturbed original.
- Sets labels, legends, and saves the plot to a PNG file.

**Why:**  
Provides visual insight into how perturbations affect the log likelihood distributions of texts.

---


## 23. `get_perturbation_results(span_length=10, n_perturbations=1, n_samples=500)`
**Purpose:**  
Runs the perturbation experiment by generating perturbed versions of original and sampled texts, then computes log likelihoods for all.

**How it works:**
- Loads the mask model and sets random seeds for reproducibility.
- Uses a partial function to set up the perturbation method.
- Perturbs both sampled and original texts multiple times.
- Asserts the correct number of perturbed samples.
- For each text, stores the original, sampled, and all perturbed versions.
- Loads the base model and computes log likelihoods for all texts and their perturbations.
- Stores means and standard deviations of log likelihoods for further analysis.

**Why:**  
Central to the DetectGPT pipeline, this function prepares all data needed for evaluating the effect of perturbations on model likelihoods.

---

## 24. `run_perturbation_experiment(results, criterion, span_length=10, n_perturbations=1, n_samples=500)`
**Purpose:**  
Evaluates the discriminative power of a chosen criterion (e.g., difference or z-score of likelihoods) on the results of a perturbation experiment.

**How it works:**
- Computes prediction scores for real and sampled texts using the specified criterion.
- Handles edge cases (e.g., zero standard deviation).
- Calculates ROC and precision-recall metrics.
- Prints and returns a dictionary with all relevant metrics and results.

**Why:**  
Quantifies how well the chosen criterion separates real from generated texts, providing key metrics for DetectGPT evaluation.

---

## 25. `run_baseline_threshold_experiment(criterion_fn, name, n_samples=500)`
**Purpose:**  
Runs a baseline experiment using a simple criterion function (e.g., likelihood, rank, entropy) to score texts.

**How it works:**
- Sets random seeds for reproducibility.
- For each batch, computes criterion scores for original and sampled texts.
- Aggregates predictions and computes ROC and precision-recall metrics.
- Prints and returns a dictionary with all relevant metrics and results.

**Why:**  
Provides baseline metrics for comparison against the perturbation-based methods, helping to contextualize DetectGPT’s performance.

---

## 26. `strip_newlines(text)`
**Purpose:**  
Removes all newlines from a text, replacing them with spaces.

**How it works:**
- Splits the text by whitespace and rejoins with single spaces.

**Why:**  
Ensures text is in a clean, single-line format for processing and modeling.


## 27. `trim_to_shorter_length(texta, textb)`
**Purpose:**  
Trims two input texts so that both are shortened to the same (shorter) length, measured in words.

**How it works:**
- Splits both `texta` and `textb` into lists of words (using spaces as delimiters).
- Determines the length of the shorter text (in words).
- Truncates both texts to this shorter length by slicing the word lists.
- Joins the truncated word lists back into strings and returns them.

**Why:**  
Ensures that two texts are directly comparable by length, which is important for fair evaluation or paired operations (e.g., when comparing original and generated samples).

---

## 28. `truncate_to_substring(text, substring, idx_occurrence)`
**Purpose:**  
Truncates the input `text` at the position of the `idx_occurrence`-th appearance of a given `substring`. If the substring does not occur enough times, returns the original text.

**How it works:**
- Asserts that `idx_occurrence` is greater than 0.
- Iterates through the text, searching for the next occurrence of `substring` each time, updating the index.
- After finding the `idx_occurrence`-th occurrence, slices the text up to that index (not including the substring itself).
- If the substring does not occur enough times, returns the original text.

**Why:**  
Useful for extracting a prefix of a text up to a certain repeated marker or delimiter, which can be important for prompt construction or data cleaning.

---

## 29. `generate_samples(raw_data, batch_size)`
**Purpose:**  
Generates a set of samples from the base model for a given set of raw input texts, batching the process for efficiency and optionally applying pre-perturbations.

**How it works:**
- Sets random seeds for reproducibility (PyTorch and NumPy).
- Initializes a dictionary with keys `original` and `sampled` to store the input and generated texts.
- Iterates over the raw data in batches of size `batch_size`.
- For each batch:
	- Appends the original texts to the `original` list.
	- Uses the model to generate samples for each input and appends them to the `sampled` list.
- If pre-perturbation is enabled (via `args.pre_perturb_pct`), applies a perturbation function to the sampled texts before storing them.
- Returns the dictionary containing both the original and sampled texts.

**Why:**  
Provides a standardized way to generate and store both original and model-generated samples, supporting downstream experiments and analyses.

---

## 30. `generate_data(dataset, key)`
**Purpose:**  
Loads, cleans, filters, and prepares a dataset for use in experiments, returning a dictionary of original and sampled texts.

**How it works:**
- Loads the data from a custom dataset or a default source, depending on the dataset name.
- Removes duplicate entries deterministically (preserving order).
- Strips whitespace and removes newlines from each example.
- For certain datasets, keeps only examples with more than 250 words.
- Shuffles the data with a fixed seed for reproducibility.
- Truncates the dataset to the first 5,000 examples for efficiency.
- Tokenizes the data and keeps only examples with 512 tokens or fewer (to ensure compatibility with the mask model and to filter out low-quality content).
- Prints statistics about the remaining data (number of samples, average length).
- Calls `generate_samples` to produce the final dictionary of original and sampled texts, using the specified batch size and sample count.

**Why:**  
Ensures that the data used for experiments is clean, deduplicated, appropriately sized, and ready for model processing, which is critical for reliable and reproducible results.


## 31. `load_base_model_and_tokenizer(name)`
**Purpose:**  
Loads the base language model and its tokenizer from a given model name, handling both local and OpenAI models, and returns them for use in downstream tasks.

**How it works:**
- Checks if an OpenAI model is being used. If so, returns `None` for the base model (since inference will be via API).
- If using a local model:
	- Loads the model and tokenizer from the HuggingFace Transformers library, using the provided `name` and any optional arguments (e.g., cache directory, dataset-specific options).
	- Sets the tokenizer's padding token to the end-of-sequence token for consistency.
- Returns the loaded model and tokenizer.

**Why:**  
Provides a unified interface for loading models and tokenizers, abstracting away the differences between local and API-based models, and ensuring correct configuration for downstream use.

---

## 32. `eval_supervised(data, model)`
**Purpose:**  
Evaluates a supervised classification model (e.g., a fine-tuned transformer) on a dataset, computing standard metrics for real and generated samples.

**How it works:**
- Prints a message indicating the start of evaluation.
- Loads the sequence classification model and tokenizer from HuggingFace, moving the model to the GPU.
- Prepares the real and fake (sampled) data for evaluation.
- Runs the model in evaluation mode (no gradients) to get predictions for both real and fake samples.
- Computes ROC and precision-recall metrics using the predictions.
- Prints the resulting AUC scores.
- Frees GPU memory by deleting the model and clearing the cache.
- Returns a dictionary with the model name, predictions, info, metrics, and loss.

**Why:**  
Allows for quantitative comparison of supervised detectors against unsupervised or perturbation-based methods, providing a baseline or upper bound for detection performance.

---

## 33. Main Method (`if __name__ == '__main__':`)
**Purpose:**  
Serves as the entry point for the script, handling argument parsing, environment setup, and orchestration of the main experimental pipeline.

**How it works:**
- Sets the computation device (e.g., CUDA for GPU).
- Defines and parses a comprehensive set of command-line arguments, covering dataset selection, model configuration, perturbation parameters, sampling options, and more.
- Handles OpenAI API key setup if needed.
- Records the start date and time for experiment tracking.
- Defines the output folder for saving results, using a timestamp and model names for uniqueness.
- (The rest of the main method, not shown in the snippet, would typically include: loading data, running experiments, saving results, and possibly plotting or logging outputs.)

**Why:**  
Centralizes configuration and execution logic, making the script flexible, reproducible, and easy to run with different settings from the command line.

---

## Summary Table

| # | Function         | Main Task                                   | Key Actions                                                                 |



| #  | Function                  | Main Task                                   | Key Actions                                                                 |
|----|---------------------------|---------------------------------------------|------------------------------------------------------------------------------|
| 1  | `load_base_model`         | Move base model to GPU                      | Moves mask model to CPU, base model to GPU, times operation                  |
| 2  | `load_mask_model`         | Move mask model to GPU                      | Moves base model to CPU, mask model to GPU (unless random fills), times op   |
| 3  | `tokenize_and_mask`       | Randomly mask spans in text                 | Splits text, computes number of spans, masks spans, replaces with `<extra_id_N>`, returns   |
| 4  | `count_masks`             | Count mask tokens in texts                  | Splits text, counts tokens starting with `<extra_id_`                        |
| 5  | `replace_masks`           | Fill masks using mask model                 | Tokenizes, generates fills, decodes output                                   |
| 6  | `extract_fills`           | Extract generated fills from model outputs  | Cleans text, splits by mask tokens, trims whitespace                         |
| 7  | `apply_extracted_fills`   | Insert fills into masked text               | Replaces mask tokens with fills, reconstructs perturbed text                 |
| 8  | `perturb_texts_`          | Perturb batch of texts                      | Masks, fills, retries if needed, supports random/model fills                 |
| 9  | `perturb_texts`           | Batch perturbation                          | Splits into chunks, calls `perturb_texts_`, aggregates results               |
| 10 | `drop_last_word`          | Remove last word from text                  | Splits, removes last word, rejoins                                          |
| 11 | `_openai_sample`          | Sample from OpenAI API                      | Optionally trims prompt, calls API, concatenates result                      |
| 12 | `sample_from_model`       | Sample from model with context              | Uses first tokens as context, ensures min length, supports OpenAI/local      |
| 13 | `get_likelihood`          | Compute average log-likelihood              | Reshapes logits/labels, computes log-probs, averages                        |
| 14 | `get_ll`                  | Log-likelihood for single text              | Uses OpenAI or local model, averages log-probs                               |
| 15 | `get_lls`                 | Log-likelihoods for list of texts           | Batch scoring, supports OpenAI/local, parallelizes if needed                 |
| 16 | `get_rank`                | Average rank of tokens                      | Finds rank of each token, averages, supports log-rank                        |
| 17 | `get_entropy`             | Average entropy of token predictions        | Computes entropy for each token, averages                                    |
| 18 | `get_roc_metrics`         | ROC curve metrics for classification        | Combines predictions, computes ROC curve and AUC                             |
| 19 | `get_precision_recall_metrics` | Precision-recall metrics for classification | Combines predictions, computes PR curve and AUC                              |
| 20 | `save_roc_curves`         | Save ROC curve plots                        | Plots ROC curves for experiments, saves PNG                                  |
| 21 | `save_ll_histograms`      | Save log likelihood histograms              | Plots histograms for log likelihoods, saves PNG                              |
| 22 | `save_llr_histograms`     | Save log likelihood ratio histograms        | Plots histograms for LLRs, saves PNG                                         |
| 23 | `get_perturbation_results`| Run perturbation experiment                 | Perturbs texts, computes log likelihoods, aggregates results                 |
| 24 | `run_perturbation_experiment` | Evaluate criterion on perturbation results | Computes scores, metrics, returns experiment results                         |
| 25 | `run_baseline_threshold_experiment` | Run baseline experiment                   | Scores texts with criterion, computes metrics, returns results               |
| 26 | `strip_newlines`          | Remove newlines from text                   | Replaces newlines with spaces, cleans text                                   |

---

These functions are essential for the span-masking and filling process used in DetectGPT-style perturbations, and are tailored for models like T5 that use `<extra_id_N>` tokens for span masking. They automate the process of masking, filling, and reconstructing perturbed texts for downstream analysis.
