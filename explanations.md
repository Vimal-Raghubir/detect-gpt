
# Explanations of the First 11 Functions in `run.py`

This document provides an in-depth breakdown of the first eleven functions in the `detect-gpt/run.py` file. Each function is explained in terms of its purpose, how it works, and its role in the overall pipeline.

---

## 1. `load_base_model()`
**Purpose:**  
Moves the base language model (e.g., GPT-2) to the GPU for faster computation, and moves the mask model (if it exists) back to CPU to free up GPU memory.

**How it works:**
- Prints a message indicating the base model is being moved to GPU.
- Records the start time for timing the operation.
- Tries to move the `mask_model` to CPU (if it exists) to free GPU memory. If `mask_model` is not defined, it ignores the error.
- If not using an OpenAI model (i.e., using a local model), moves `base_model` to the GPU device specified by `DEVICE`.
- Prints how long the operation took.

**Why:**  
Switching models between CPU and GPU helps manage limited GPU memory, especially when working with large models.

---

## 2. `load_mask_model()`
**Purpose:**  
Moves the mask-filling model (e.g., T5) to the GPU, and moves the base model back to CPU to save GPU memory.

**How it works:**
- Prints a message indicating the mask model is being moved to GPU.
- Records the start time.
- If not using an OpenAI model, moves the `base_model` to CPU to free up GPU memory.
- If not using random fills (i.e., actually using the mask model), moves `mask_model` to the GPU.
- Prints how long the operation took.

**Why:**  
This function is the counterpart to `load_base_model()`, ensuring only one large model is on the GPU at a time to avoid memory issues.

---

## 3. `tokenize_and_mask(text, span_length, pct, ceil_pct=False)`
**Purpose:**  
Randomly masks out spans of words in the input text, replacing them with special tokens (`<extra_id_0>`, `<extra_id_1>`, etc.) for use in span-masked language modeling (e.g., T5).

**How it works:**
1. **Tokenization:** Splits the input text into tokens (by spaces).
2. **Calculate Number of Spans:** Computes how many spans to mask based on the percentage (`pct`), the length of each span (`span_length`), and a buffer size. Optionally rounds up the number of spans if `ceil_pct` is `True`. Converts the number of spans to an integer.
3. **Masking Loop:** While the number of masked spans is less than the target, randomly selects a start index for a span, checks a buffer region around the span to ensure it doesn’t overlap with an existing mask, and if the region is clear, replaces the span with a placeholder (`<<<mask>>>`).
4. **Replace Placeholders with Special Tokens:** Iterates through the tokens, replacing each `<<<mask>>>` with a unique `<extra_id_N>` token, incrementing `N` for each mask. Asserts that the number of filled masks matches the intended number.
5. **Return:** Joins the tokens back into a string and returns the masked text.

**Why:**  
This prepares text for models like T5, which are trained to fill in masked spans denoted by `<extra_id_N>` tokens. The masking is random and non-overlapping, simulating the span corruption used in T5-style pretraining.

---

## 4. `count_masks(texts)`
**Purpose:**  
Counts the number of mask tokens (e.g., `<extra_id_0>`, `<extra_id_1>`, etc.) in each text in a list.

**How it works:**
- For each text in the input list `texts`, it splits the text into tokens (by spaces).
- It counts how many tokens start with `<extra_id_`.
- Returns a list of counts, one for each input text.

**Why:**  
This is used to determine how many masked spans are present in each text, which is important for later filling or processing steps.

---

## 5. `replace_masks(texts)`
**Purpose:**  
Fills in the masked spans in each text using the mask model (e.g., T5), generating new text with the masks replaced by model-generated content.

**How it works:**
- Uses `count_masks` to determine how many masks are in each text.
- Computes a `stop_id` so the model knows when to stop generating (based on the highest mask index).
- Tokenizes the input texts for the mask model.
- Calls the mask model’s `generate` method to fill in the masks, with sampling and stopping parameters.
- Decodes the generated outputs back into text, keeping special tokens.

**Why:**  
This function automates the process of filling in masked spans with plausible content, as part of the perturbation pipeline.

---

## 6. `extract_fills(texts)`
**Purpose:**  
Extracts the content generated by the mask model that fills in each masked span.

**How it works:**
- Cleans up each text by removing `<pad>` and `</s>` tokens and stripping whitespace.
- Uses a regex pattern to split the text at each `<extra_id_N>` token, extracting the segments between them (which are the fills).
- Removes leading/trailing whitespace from each fill.
- Returns a list of lists: each sublist contains the fills for one text.

**Why:**  
After the mask model generates text, this function isolates the actual content that was generated to fill each mask, so it can be inserted back into the original text.

---

## 7. `apply_extracted_fills(masked_texts, extracted_fills)`
**Purpose:**  
Replaces each `<extra_id_N>` mask token in the masked texts with the corresponding generated fill, reconstructing the perturbed text.

**How it works:**
- Splits each masked text into tokens.
- For each text, checks if the number of fills matches the number of masks.
- If so, replaces each mask token with its fill.
- Joins tokens back into a string and returns the list of perturbed texts.

**Why:**  
This function reconstructs the final perturbed text by inserting the generated content into the masked positions.

---

## 8. `perturb_texts_(texts, span_length, pct, ceil_pct=False)`
**Purpose:**  
Perturbs a batch of texts by masking and filling spans, either using a model or random fills.

**How it works:**
- If not using random fills:
	- Masks spans, generates fills, extracts them, and applies them.
	- Retries if the model fails to generate the correct number of fills.
- If using random fills:
	- Either replaces tokens with random tokens or fills masks with random words from a dictionary.

**Why:**  
This is the core function for generating perturbed versions of input texts for analysis.

---

## 9. `perturb_texts(texts, span_length, pct, ceil_pct=False)`
**Purpose:**  
Handles batching for perturbing large numbers of texts efficiently.

**How it works:**
- Splits the input texts into chunks (to avoid memory issues).
- Calls `perturb_texts_` on each chunk and aggregates the results.

**Why:**  
Allows the perturbation process to scale to large datasets by processing in manageable batches.

---

## 10. `drop_last_word(text)`
**Purpose:**  
Removes the last word from a given text string.

**How it works:**
- Splits the input text into words (by spaces).
- Removes the last word.
- Joins the remaining words back into a string.

**Why:**  
Used to trim prompts, especially before sending them to a language model for continuation.

---

## 11. `_openai_sample(p)`
**Purpose:**  
Generates a text continuation from the OpenAI API, using the given prompt.

**How it works:**
- Optionally drops the last word from the prompt (unless the dataset is 'pubmed').
- Sets up parameters for the OpenAI API call, including model engine and sampling options.
- Calls the API to generate a continuation.
- Returns the original prompt concatenated with the generated text.

**Why:**  
Used to sample text from OpenAI models for evaluation or data augmentation.

---

## Summary Table

| # | Function         | Main Task                                   | Key Actions                                                                 |
|---|------------------|---------------------------------------------|------------------------------------------------------------------------------|
| 1 | `load_base_model`    | Move base model to GPU                   | Moves mask model to CPU, base model to GPU, times operation                  |
| 2 | `load_mask_model`    | Move mask model to GPU                   | Moves base model to CPU, mask model to GPU (unless random fills), times op   |
| 3 | `tokenize_and_mask`  | Randomly mask spans in text              | Splits text, computes number of spans, masks spans, replaces with `<extra_id_N>`, returns   |
| 4 | `count_masks`        | Count mask tokens in texts               | Splits text, counts tokens starting with `<extra_id_`                        |
| 5 | `replace_masks`      | Fill masks using mask model              | Tokenizes, generates fills, decodes output                                   |
| 6 | `extract_fills`      | Extract generated fills from model outputs| Cleans text, splits by mask tokens, trims whitespace                         |
| 7 | `apply_extracted_fills` | Insert fills into masked text           | Replaces mask tokens with fills, reconstructs perturbed text                 |
| 8 | `perturb_texts_`     | Perturb batch of texts                   | Masks, fills, retries if needed, supports random/model fills                 |
| 9 | `perturb_texts`      | Batch perturbation                       | Splits into chunks, calls `perturb_texts_`, aggregates results               |
|10 | `drop_last_word`     | Remove last word from text               | Splits, removes last word, rejoins                                          |
|11 | `_openai_sample`     | Sample from OpenAI API                   | Optionally trims prompt, calls API, concatenates result                      |

---

These functions are essential for the span-masking and filling process used in DetectGPT-style perturbations, and are tailored for models like T5 that use `<extra_id_N>` tokens for span masking. They automate the process of masking, filling, and reconstructing perturbed texts for downstream analysis.
